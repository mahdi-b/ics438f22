{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf652b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Some) Problems with The Approach used in Assignment 1\n",
    "\n",
    "* Lack of overlap between a large collection of documents\n",
    "  * The more disparate documents we consider, the smaller the intersection\n",
    "  * Can substantially hamper clustering and classification\n",
    "\n",
    "* Considering all the non-stop words shared between the documents (union) leads to an unnecessarily lot of very large datasets\n",
    "  * Many words that share a prefix, e.g., leave, leaving, left all lead to different counts despite potentially referencing the same concept\n",
    "\n",
    "* The semantic meaning of words is not considered\n",
    "  * We want the words Bank in the following sentences to have different meanings and hence counts. \n",
    "    * \"She works at the bank across the street\" vs. \"houses on the bank of the river flooded due to. a storm surge\"\n",
    "  * We want the words Bank and financial institution to have the same meaning. E.g.:\n",
    "    * \"She works at the financial institution across the street\" vs. \"She's been working at the bank for over 5 years\"\n",
    "  * This is (was?) a *very hard* problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cf84a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of Words in a Text \n",
    "\n",
    "* The frequency distribution of words in a language follows Zipf's law\n",
    "  * Just FYI: this makes computign statistics rather difficult or impossible\n",
    "  \n",
    "![](https://www.dropbox.com/s/neydq8wi2kqqof3/zipf_law.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbf775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document Similarity: Deriving A Matching Score\n",
    "\n",
    "* Document Search in Information Retrieval (IR) is typically concerned with scoring matches based on some order of perceived importance=\n",
    "  * Finding similar documents in a corpus\n",
    "  * Ideally, we want the search to rank the hits (documents found) by their similarity to the query  \n",
    "  We wish to return in order the documents most likely to be useful to the individual searching\n",
    "\n",
    "* How can we rank-order the documents in the collection with respect to a query?\n",
    "  * Assign a score – say in [0, 1] – to each document\n",
    "  * This score measures how well hit and query “match”\n",
    "\n",
    "* Document Retrieval is a field with a long history of creative solutions to similar problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c78ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document Similarity: Deriving A Matching Score\n",
    "\n",
    "\n",
    "\n",
    "* Consider a trivial search query consisting of a single term \n",
    "\n",
    "* If the query term does not occur in the document, score should be 0\n",
    "\n",
    "* The more frequent the query term in the document, the higher the score (should be)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723aaa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jaccard Coefficient\n",
    "\n",
    "\n",
    "* Recall that the Jaccard Coefficient is a commonly used measure of overlap between two sets A and B\n",
    "  * The number of overlaps between A and B normalized by all the words in A and B.\n",
    "* Does not require A and B to have the same size.\n",
    "* Always assigns a number between 0 and 1.\n",
    "\n",
    "* Shortcoming:\n",
    "* Does not consider term frequency\n",
    "* Jaccard doesn’t consider the fact that rare terms in a collection are more informative than frequent terms. \n",
    "  * The reason why the intersection is not a good idea\n",
    "\n",
    "* We need a more sophisticated way of normalizing for length (instead of $|A \\cup B|$)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-document Count Matrices\n",
    "\n",
    "* In a count matrix, each word is represented with its frequency (count in a document)\n",
    "  * As was mentioned in Assignment 2, this is called the bag of words model \n",
    "* Does not consider the order of words in the document\n",
    "\n",
    "* `John is quicker than Mary` and `Mary is quicker than John` have the exact same vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf88fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term frequency `tf`\n",
    "\n",
    "* Formally, the term frequency $tf_{t,d}$ of term $t$ in document $d$ is defined as the number of times that $t$ occurs in $d$.\n",
    "*  The $tf$ is typically informative of the quality of a match, however, differences in $tf$ do not directly determine importance. E.g.:\n",
    "\n",
    "  * A document with 10 occurrences of the term is more relevant than a document with 1 occurrence of the term, but not 10 times more relevant.\n",
    "\n",
    "  * I.e., relevance does not increase proportionally with term frequency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e540",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log-frequency weighting\n",
    "\n",
    "* The log-frequency weight of term $t$ in $d$ is\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1+\\log_{10}\\mbox{tf}_{t,d} & \\mbox{if } \\mbox{tf}_{t,d} > 0\\\\\n",
    "        0 & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "```0 → 0, 1 → 1, 2 → 1.3, 10 → 2, 1000 → 4, etc.```\n",
    "\n",
    "* Score for a document-query pair: sum over terms t in both q and d:\n",
    "\n",
    "$$\n",
    "\\mbox{score} = \\sum_{t\\in \\cap d}(1+\\log_{10}\\mbox{tf}_{t,d})\n",
    "$$\n",
    "\n",
    "* The score is 0 if none of the query terms is present in the document.\n",
    "\n",
    "* Despite other variations, the essence of the calculations remains the same\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e78dad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document frequency\n",
    "\n",
    "* We still have the issue of rare terms\n",
    "  * Rare terms are more informative than frequent terms\n",
    "    * Recall stop words\n",
    "* Consider a term in the query that is rare in the collection (e.g., arachnocentric)\n",
    "  * A document containing this term is very likely to be relevant to the query arachnocentric\n",
    "  * The term is very likely to be relevant to clustering the docuements  \n",
    "  * Thus, we want a high weight for rare terms like arachnocentric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0aaf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document Frequency Contd'\n",
    "\n",
    "* Frequent terms are less informative than rare terms.\n",
    "* Consider a query term that is frequent in the collection (e.g., `high`, `increase`, `true`)\n",
    "  * Based on the $tf$ score alone, a document containing such a term is more likely to be relevant than a document that doesn’t\n",
    "  * But it’s not a sure indicator of relevance.\n",
    "* To assess the frequency of a work in a document, we'll compute (normlaize by) the document frequency `df` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `idf` Weight\n",
    "\n",
    "$\\mbox{df}_{t,d}$ frequency of $t$ in a single document $d$\n",
    "* $\\mbox{df}_t$ is an inverse measure of the informativeness of $t$\n",
    "    * $\\mbox{df}_t \\le N$, where $N$ is the total number of documents.\n",
    "\n",
    "* We define the $\\mbox{idf}$ (inverse document frequency) of $t$ by\n",
    "$$\n",
    "idf_t = log_{10}(N/\\mbox{df}_t)\n",
    "$$\n",
    "* We'd rather work with ther inverse to avoid very small numbers since $N>>\\mbox{df}_f$\n",
    "* We use the `log` to dampen the effect of idf\n",
    "  * Particularly useful when we have a large number of documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55756d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `tf-idf` Weighting\n",
    "\n",
    "* The `tf-idf` weight of a term is the product of its `tf` weight and its `idf` weight.\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "w_{t,d} &= \\mbox{tf}_{t,d} \\times \\mbox{idf}_t\\\\\n",
    "&= log(1+\\mbox{tf}_{t,d}) \\times log(N/\\mbox{df}_t)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "* A popular weighting scheme in information retrieval\n",
    "  * Alternative names: tf.idf, tf x idf\n",
    "\n",
    "* Increases with the number of occurrences within a document\n",
    "* Increases with the rareness of the term in the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955c6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Score for a Document Given a Query\n",
    "\n",
    "\n",
    "$$\n",
    "Score(Q, T) = \\sum_{t\\in Q\\cap T} \\mbox{tf}.\\mbox{idf}_{t,d}\n",
    "$$\n",
    "\n",
    "* There are many variants\n",
    "  * How `tf` is computed (with/without logs)\n",
    "  * Whether the terms in the query are also weighted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfc8d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `tf-idf` for Feature Engineering\n",
    "* Each document is represented by a real-valued vector of $\\mbox{tf-idf}$ weights $\\in R^{|V|}$\n",
    "\n",
    "![](https://www.dropbox.com/s/1bx77e488ee6wek/count_tf_idf.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce64927",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Documents, Vectors and Space Proximity in IR\n",
    "\n",
    "\n",
    "* **Key idea 1**: Do the same for queries: represent them as vectors in the space\n",
    "* **Key idea 2**: Rank documents according to their proximity to the query in this space\n",
    "  * proximity = similarity of vectors\n",
    "* First cut: distance between two points, i.e., distance between the end points of the two vectors\n",
    "* Euclidean distance is a bad idea when the instances have differet lengths\n",
    "  * The Euclidean distance is large for vectors of different lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e981d76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Documents, Vectors and Space Proximity - Cont'd\n",
    "\n",
    "* Thought experiment: take a document $d$ and append it to itself. Call this document $d′$.\n",
    "  * “Semantically” $d$ and $d′$ have the same content\n",
    "  * The Euclidean distance between the two documents can be quite large\n",
    "\n",
    "* The angle between the two documents is 0, corresponding to maximal similarity.\n",
    "\n",
    "* Key idea: Rank documents according to angle with query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac789fac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_1 = \"\"\"The king hath happily received, Macbeth,\n",
    "The news of thy success; and when he reads\n",
    "Thy personal venture in the rebels' fight,\n",
    "His wonders and his praises do contend\n",
    "Which should be thine or his: silenced with that,\n",
    "In viewing o'er the rest o' the selfsame day,\n",
    "He finds thee in the stout Norweyan ranks,\n",
    "Nothing afeard of what thyself didst make,\n",
    "Strange images of death. As thick as hail\n",
    "Came post with post; and every one did bear\n",
    "Thy praises in his kingdom's great defence,\n",
    "And pour'd them down before him.\"\"\"\n",
    "\n",
    "doc_1 = doc_1.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b05916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \n",
    "eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut \n",
    "enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor \n",
    "in reprehenderit in voluptate velit esse cillum dolore eu fugiat\n",
    "nulla pariatur.\n",
    "\"\"\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5cb24d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "corpus = [doc_1, doc_1 + lorem_ipsum]\n",
    "vocabulary = ['king', 'happily', 'and', \"thy\", \"ipsum\"]\n",
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "tfidf_vec = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d88a8390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 4, 3],\n",
       "        [1, 1, 4, 3]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3014692a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a651dd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.19245009, 0.19245009, 0.76980036, 0.57735027],\n",
       "        [0.19245009, 0.19245009, 0.76980036, 0.57735027]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec115d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.19245009, 0.19245009, 0.76980036, 0.57735027],\n",
       "        [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ef71d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Angles to Cosines\n",
    "\n",
    "* In information retrieval, the following two notions are equivalent.\n",
    "  * Rank documents in decreasing order of the angle between query and hit\n",
    "  * Rank documents in increasing order of cosine(query,hit)\n",
    "\n",
    "* Cosine is a monotonically decreasing function for the interval [0o, 180o]\n",
    "\n",
    "![](https://www.dropbox.com/s/lpq4vvnlnmz0oxw/cosine.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f75b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Length Normalization\n",
    "\n",
    "* A vector can be (length-) normalized by dividing each of its components by its length \n",
    "  * We commonly use the $L2$ norm:\n",
    "\n",
    "* Dividing a vector by its $L2$ norm makes it a unit (length) vector\n",
    "\n",
    "  * Effect on the two documents $d$ and $d′$ (d appended to itself) have identical vectors after length-normalization.\n",
    "  * Thus, long and short documents now have comparable weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d7d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similairity\n",
    "\n",
    "* $q_i$ is the `tf-idf` weight of term `i` in the query\n",
    "* `d_i` is the tf-idf weight of term `i` in the document\n",
    "\n",
    "![](https://www.dropbox.com/s/4x1fb50xiqidmnf/cos_equation.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036e0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similarity Illustrated\n",
    "\n",
    "![](https://www.dropbox.com/s/4inqt6nf9mfz6h9/cosine_similarity.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0fac1",
   "metadata": {},
   "source": [
    "### Example \n",
    "* Books: \"Sense and Sensibility\", \"Pride and Prejudice\", \"Wuthering Heights?\".\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/z28xu8xxhuv8ll5/example_books.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "```\n",
    "cos(SaS,PaP) ≈ 0.789 × 0.832 + 0.515 × 0.555 + 0.335 × 0.0 + 0.0 × 0.0 ≈ 0.94\n",
    "cos(SaS,WH) ≈ 0.79\n",
    "cos(PaP,WH) ≈ 0.69\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a65d3d",
   "metadata": {},
   "source": [
    "### `tf-idf` Weighing  Variants\n",
    "\n",
    "* Just an FYI\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/r88cmbmaqyk7hcp/weighting_schemes.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "* identifies components of the SMART notation: combination in use in a search engine (ddd.qqq)\n",
    "  * ex lnc.ltc\n",
    "* See [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) for more info if you're interested in the topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498da61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
