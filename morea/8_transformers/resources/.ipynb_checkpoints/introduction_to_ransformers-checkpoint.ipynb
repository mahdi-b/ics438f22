{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199dfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c704bdb",
   "metadata": {},
   "source": [
    "### About Transformers\n",
    "\n",
    "* Transformers are an architecture (family) of language models\n",
    "  * In the same way that CNNs and RNNs are common architectures for working with image or sequential data. \n",
    "\n",
    " * Trained on large amounts of raw text in a self-supervised fashion. \n",
    "    * Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. \n",
    "\n",
    "* Resulting models develop statistical understanding of the language they has been trained on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0b4f",
   "metadata": {},
   "source": [
    "### Tranforms are Big Models\n",
    "\n",
    "* Transformers are typically very large models\n",
    "\n",
    "![](https://miro.medium.com/max/1338/1*40VA19kG5zUmTj-AOnh47A.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce14d5",
   "metadata": {},
   "source": [
    "### Transformers: an Encoder and a Decoder\n",
    "\n",
    "![](https://www.dropbox.com/s/g3vfmxmb926l4hn/encoder_decoder_arch.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72858d6c",
   "metadata": {},
   "source": [
    "### Transformers: an Encoder and a Decoder\n",
    "\n",
    "* The encoder finds an appropriate representation of the input\n",
    "\n",
    "* The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence.\n",
    "\n",
    "* It turns out that the encoder can used independently of the decoder\n",
    "  * In this course, we will focus solely on the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a849a",
   "metadata": {},
   "source": [
    "### Transformers Architecture: The encoder\n",
    "\n",
    "* As discussed, Word2Vec can be used as a context-specific \"dictionary\" to assign embeddings to words\n",
    "  * This is a problem since the meaning of a word depends on its' content\n",
    "  * e.g.: English language is full of homonyms \n",
    "    * \"He banks as FHB\" vs. \"He banks on her support to win the election\"\n",
    "    * \"He wore a bow tie to the event\"  vs. \"He used a bow and arrows to hunt the prey.\"\n",
    "\n",
    "* The encoder builds a *contextualized* representation an input. \n",
    "  * This means that the model can acquire understanding from the input.\n",
    "* One of the most popular encoder models is BERT\n",
    "  * Bidirectional Encoder Representations from Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953ce8a",
   "metadata": {},
   "source": [
    "### Transformers Architecture: Example Encoder -- BERT\n",
    "\n",
    "* BERT is a way to contextually encode a word\n",
    "  * The embedding of the word is context dependent. \n",
    "  * \"He banks as FHB\" vs. \"He banks on her support to win the election\"\n",
    "  * The value of \"banks\" takes into account the value of the words around it.\n",
    "    \n",
    "* Unlike Word2Vec, BERT does not just operate like a dictionary\n",
    "\n",
    "* Size of the generated embedding depends on the architecture\n",
    "  * For BERT base , the size 768\n",
    "\n",
    "* BERT embedding is said to hold the meaning within the text\n",
    "  * BERT tokenization, so not \"1 word = 1 embedding\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e482ec",
   "metadata": {},
   "source": [
    "### More encoders\n",
    "\n",
    "* There are dozens of different encoder architectures. For example:\n",
    "    \n",
    "* Thre are also dozens or modes that fine-tune BERT to specific domains\n",
    " * FinBERT(Finance) https://github.com/ProsusAI/finBERT\n",
    " * med-BERT (medica field) https://github.com/ProsusAI/finBERT\n",
    " * Sci-BERT: Scientific Text Bert (https://aclanthology.org/D19-1371.pdf)\n",
    "  ...        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40bb49",
   "metadata": {},
   "source": [
    "### Transformers Architecture: The Decoder\n",
    "\n",
    "* The decoders work similarly to an encoder \n",
    " * Unlike the encoder, the decoder uses masked self-attention. \n",
    "   * Unlike the encoder, it can only see the words on one side (ex. left)\n",
    "\n",
    "* The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence.\n",
    "* Word at position $i$ depends only on words at positions $i-1$ \n",
    "  * This means that the model is optimized for generating outputs.\n",
    "* Decoders are not as relevant for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638cfaf",
   "metadata": {},
   "source": [
    "### Popular Transformers\n",
    "\n",
    "* GPT and GPT-2: transformer-based language model (GPT-2 has 1.5 billion parameters)\n",
    "  * Trained on 8 million web pages\n",
    "* GPT-3, or the third generation GPT transormer \n",
    "  * 175 billion learning parameters\n",
    "  * Incredible (scary) good at a dizzying number of tasks\n",
    "  * Generating Web Component or SQL code from a language query\n",
    "  * https://github.com/features/copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc66ec",
   "metadata": {},
   "source": [
    "### Some NLP Tasks Empowered by Transforems\n",
    "* Feature Extraction\n",
    "  * Getting the vector embedding of word, sentence, paragraph or even document\n",
    "* Question answering\n",
    "* Sentiment analysis\n",
    "* Summarization\n",
    "* Zero shot classification\n",
    "* Named entity recognition\n",
    "* Etc.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16af25",
   "metadata": {},
   "source": [
    "### Huggin Face Model Hub\n",
    "\n",
    "* The Apple Store of transformer-based Language models\n",
    "  * Some newer transformer-based image models as well\n",
    "\n",
    "* Dozens of pre-trained models  \n",
    "  * Default models pretrained for specific tasks\n",
    "* Support for over 100 languages.\n",
    "* APIs to download and use those pre-trained models on a given text\n",
    " * Complete pipeline, including text processing\n",
    " * Often painfully difficult to process the data which is often model specific\n",
    "* Possibility to fine tune the model for custom data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91bece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahdi/miniconda3/envs/s3-next/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b07732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998667240142822}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"ICS 438 is an amazing course. Everyone should take it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092f4938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991720914840698}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"What a super boring movie. Never going to recommend it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d45cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_classify = [\n",
    "                            \"I really like the new design. Your app rocks!\",\n",
    "                            \"What a mess this site is to navigate\",\n",
    "                            \"very confusing to get anything done on this new redesign\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ece8ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998397827148438},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997926354408264},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996102452278137}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sentences_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa393f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9991198182106018}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Positive?\n",
    "### Models are not humans -- undrstanding of the limitations is critical\n",
    "classifier(\"He went home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77947f",
   "metadata": {},
   "source": [
    "### Example: Zero Shot Classification\n",
    "\n",
    "* Zero-shot learning: solve a task despite not having received any training examples of that task.\n",
    "  * E.g.: recognizing a category of object in photos without ever having seen a photo of that kind of object before. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4cf11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The CDC is approving booster vaccine shots for everyone over the age of 18.',\n",
       " 'labels': ['healthcare', 'business', 'politics'],\n",
       " 'scores': [0.9219019412994385, 0.06307942420244217, 0.015018666163086891]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier(\n",
    "    \"The CDC is approving booster vaccine shots for everyone over the age of 18.\",\n",
    "    candidate_labels=[\"politics\", \"business\", 'healthcare'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae06d7",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "* Find the entities (such as persons, locations, or organizations) in a sentence. \n",
    " * Classify each label of the sentence to a class per entity and one class for “no entity.”\n",
    "* Default classes\n",
    "    * O means the word doesn’t correspond to any entity.\n",
    "    * PER person entity\n",
    "    * ORG: organization entit\n",
    "    * LOC: location entity\n",
    "    * MISC: miscellaneous entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff0c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<00:00, 432kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.33G/1.33G [01:07<00:00, 19.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 60.0/60.0 [00:00<00:00, 45.3kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 380kB/s]\n",
      "/Users/mahdi/miniconda3/envs/s3-next/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.9993223,\n",
       "  'word': 'San Francisco',\n",
       "  'start': 14,\n",
       "  'end': 27},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9980908,\n",
       "  'word': 'Elon Musk',\n",
       "  'start': 29,\n",
       "  'end': 38},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9525725,\n",
       "  'word': 'Tesla',\n",
       "  'start': 72,\n",
       "  'end': 77}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"Speaking from San Francisco, Elon Musk asked whether he should sell off Tesla stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580f753",
   "metadata": {},
   "source": [
    "### The Pipeline Step in Details\n",
    "\n",
    "* Word Tokenization\n",
    "* Input processing\n",
    "* Models Processing\n",
    "* Post-processsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd375795",
   "metadata": {},
   "source": [
    "### Word Encoding\n",
    "\n",
    "* Models require numerical inputs. \n",
    "\n",
    "* Naive: assign a unique value to each word in the vocabulary\n",
    "\n",
    "   * {\"aardvark\": 1, ... \"Zeuxis\": 125,452}\n",
    "\n",
    "* This approach is referred to as work tokenization\n",
    "\n",
    "  * Split on words and punctuation.\n",
    "\n",
    "  * Assign each word a unique ID \n",
    "\n",
    "* Corpus may contain hundreds of thousands of words and the dataset can be very large. \n",
    "\n",
    "```There is one count that puts the English vocabulary at about 1 million words — but that count presumably includes words such as Latin species names, prefixed and suffixed words, scientific terminology, jargon, foreign words of extremely limited English use, and technical acronyms.```[https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea453afa",
   "metadata": {},
   "source": [
    "### Character Tokenization\n",
    "\n",
    "* The other end of the spectrum, we could encode every charcater independently\n",
    "  * Our encoder need only need valid alphabet and punctation characters\n",
    "* Small encoding scheme but can encode any word in the same alphabet\n",
    "\n",
    "Issue with this approach:\n",
    "\n",
    "* Encoding for single sentence become large\n",
    "* token do no mean anything when taken separately\n",
    "  * need to be combined to generate a userful meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75df6b",
   "metadata": {},
   "source": [
    "### Word Tokenization \n",
    "* Questions:\n",
    " 1. How de encode these without explicitly accounting for every word in the language?\n",
    "\n",
    " 2. How do you encode for very similar words (car vs. cars, text vs. textual)\n",
    "\n",
    "  * Yes, we can rely on embedding to be fairly similar, but maybe we can encode words so that they match before computing embedding.\n",
    "\n",
    " 3. How should a deployed model handle previously unseen words? \n",
    "\n",
    "* Solution: subword tokenization    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad07d0",
   "metadata": {},
   "source": [
    "### Sub-word Encoding: An Intuition\n",
    "\n",
    "* Language contains hundreds of thousands of words and text is often very large \n",
    "\n",
    "  * How can we encode these words without explicitly accounting for every word in the language?\n",
    "\n",
    "    \n",
    "\n",
    "* Use tokens instead of words\n",
    "\n",
    "  * Split a word into prefix, stem and suffix\n",
    "\n",
    "  * unusually -> un + usual + ly  \n",
    "\n",
    "  * unsuspiciously -> un + suspicious + ly  \n",
    "\n",
    "* [suspicious, usual, un, ly] these 4 tokens can construct 8 words \n",
    "\n",
    "  * suspicious, usual, unsuspicious, unusual, usually, suspiciously, unusually, unsuspiciously \n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "* Expand the approach to include common stems, suffixes, and prefixes.\n",
    "\n",
    "  * Makes it easy for words with similar stems to match while keeping the vocabulary small\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5dc40",
   "metadata": {},
   "source": [
    "### Sub-word Encoding\n",
    "\n",
    "* Word should be split into meaningful subwords\n",
    "\n",
    "* Frequent words should not be split. \n",
    "\n",
    "* Rare words should be split into subwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f2b5a",
   "metadata": {},
   "source": [
    "### Sub-word Tokenization Schemes\n",
    "\n",
    "* Different models use different schemes for splitting encoding words\n",
    "  * BERT uses Word Piece: Tokenization introduced by Google.\n",
    "    * Algorithm used has not been open sources; reverse engineered in some applications\n",
    "  * ALBERT uses Unigram \n",
    "    * Substantially different form Word Piece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e343a47",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm: General Approach\n",
    "\n",
    "* A greedy algorithm that decomposes its vocabulary into chunks and retains the most frequent ones.\n",
    "\n",
    "* Builds a vocabulary containing the most frequent chunks\n",
    "\n",
    "* Given the following corpus, the algorithm proceeds as described next\n",
    "\n",
    "```Corpus =  {HuggingFace, hugging, face, hug, hugger, learning, learner, learners, learn},```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c654d79",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -1 \n",
    "![](https://www.dropbox.com/s/q1dctybrlx2y0mh/wp_1.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd444dc",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -2\n",
    "![](https://www.dropbox.com/s/f5626qe6mpdjapm/wp_2.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b9c52",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm -3\n",
    "![](https://www.dropbox.com/s/5m5n0qvn22lfwxk/wp_3.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc4184",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm - 4\n",
    "![](https://www.dropbox.com/s/4ol60txxkqy6evp/wp_4.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64a0cf",
   "metadata": {},
   "source": [
    "### Word Piece Algorithm - Matching\n",
    "\n",
    "![](https://www.dropbox.com/s/875ntqoyuhh8bg9/wp_5.png?dl=1)\n",
    "\n",
    "* Each word is encoded using a unique value\n",
    "* Input representation is encoding of its unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c1869",
   "metadata": {},
   "source": [
    "### Model to Tokenizer Mapping.\n",
    "\n",
    "* It's critical to use the correct encoding for each model we need to use. \n",
    "  * To use BERT model, we need to convert the input data using the same tokenizer it used for it's training\n",
    "    * Split the workds in the same way that the model does\n",
    "    * Use the same delimiter characters\n",
    "    * use the same token ids as the model\n",
    "\n",
    "\n",
    "* For more details, see: https://huggingface.co/transformers/tokenizer_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30110632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10683, 22559,  2734,  1110,  4348,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the bert cased model is case-sensitive: it makes a difference between english and English.\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "my_input = tokenizer(\"Word tokenization is cool!\", return_tensors=\"pt\")\n",
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6e21c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Word', 'token', '##ization', 'is', 'cool', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the \"##\" indicated the token does not occur at the start of the word.\n",
    "tokens = tokenizer.tokenize(\"Word tokenization is cool!\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337724bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word tokenization is cool !'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a3a4436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10683, 22559, 2734, 1110, 4348, 106]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89c393bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 10683, 22559, 2734, 1110, 4348, 106, 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input[\"input_ids\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c344ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47a7f16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interior',\n",
       " 'echoed',\n",
       " 'Valentine',\n",
       " 'varieties',\n",
       " 'Brady',\n",
       " 'cluster',\n",
       " 'Ever',\n",
       " 'voyage',\n",
       " '##of',\n",
       " 'deposits',\n",
       " 'ultimate',\n",
       " 'Hayes',\n",
       " 'horizontal',\n",
       " 'proximity',\n",
       " '##ás',\n",
       " 'estates',\n",
       " 'exploration',\n",
       " 'NATO',\n",
       " 'Classical',\n",
       " '##most']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[10_000:10_020]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3128a0",
   "metadata": {},
   "source": [
    "### BERT Architecture and Embeddings\n",
    "![](https://jalammar.github.io/images/bert-output-vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b10e2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant modules\n",
    "from transformers import BertModel\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "import torch\n",
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate\n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "706e96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0319,  0.4166, -0.4186,  ..., -0.3329,  0.3522,  0.1938],\n",
       "         [ 0.3837,  0.1618, -0.6934,  ...,  0.1908,  1.1430, -0.5608],\n",
       "         [ 0.7917, -0.5113, -0.0309,  ..., -0.5770,  0.1681, -0.1110],\n",
       "         ...,\n",
       "         [ 0.0486, -0.7598, -0.4104,  ..., -0.5119, -0.1877, -0.1703],\n",
       "         [ 0.5552,  0.0767, -0.4177,  ...,  0.1522, -0.3517, -0.4224],\n",
       "         [ 0.7520,  0.1612, -0.1570,  ...,  0.0096, -0.5722, -0.2309]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The bank was out of money.\"\n",
    "tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**tokenized_input)\n",
    "output[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4727ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d55433f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "##############\n",
    "# These sentences show different uses of the word 'bank' and illustrate the\n",
    "# value of contextualized embeddings\n",
    "\n",
    "texts = [\"bank\",\n",
    "         \"The river bank was flooded.\",\n",
    "         \"The bank vault was robust.\",\n",
    "         \"He had to bank on her for support.\",\n",
    "         \"The bank was out of money.\",\n",
    "         \"The bank robber was arrested.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c93fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'bank', '[SEP]']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c90155ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_word(word, tokenized_input):\n",
    "    words = tokenizer.convert_ids_to_tokens(tokenized_input) \n",
    "    return words.index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c39b4559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index_word(\"bank\", tokenized_input[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a333b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings for the target\n",
    "# word in all given contexts\n",
    "# The following is implemented using a for loop with illustration purposes only.\n",
    "target_word_position = []\n",
    "target_word_embeddings = []\n",
    "\n",
    "for text in texts:\n",
    "    tokenized_input = tokenizer(text, return_tensors=\"pt\")\n",
    "    output = model(**tokenized_input)\n",
    "    embeddings  = output.last_hidden_state\n",
    "    embeddings = torch.squeeze(embeddings, dim=0)\n",
    "    word_index = find_index_word(\"bank\", tokenized_input[\"input_ids\"][0])\n",
    "    target_word_position.append(word_index)\n",
    "    word_embedding = embeddings[word_index]\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aead88d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 4, 2, 2]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "daee8c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0ff645e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank</td>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>0.338063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>0.494098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bank</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.256140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.469941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bank</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.433940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>0.523326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.331584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.512161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The river bank was flooded.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.535616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>0.416074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.759213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The bank vault was robust.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.798656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>0.458184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>He had to bank on her for support.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.381479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The bank was out of money.</td>\n",
       "      <td>The bank robber was arrested.</td>\n",
       "      <td>0.700466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text1                               text2  \\\n",
       "0                                 bank         The river bank was flooded.   \n",
       "1                                 bank          The bank vault was robust.   \n",
       "2                                 bank  He had to bank on her for support.   \n",
       "3                                 bank          The bank was out of money.   \n",
       "4                                 bank       The bank robber was arrested.   \n",
       "5          The river bank was flooded.          The bank vault was robust.   \n",
       "6          The river bank was flooded.  He had to bank on her for support.   \n",
       "7          The river bank was flooded.          The bank was out of money.   \n",
       "8          The river bank was flooded.       The bank robber was arrested.   \n",
       "9           The bank vault was robust.  He had to bank on her for support.   \n",
       "10          The bank vault was robust.          The bank was out of money.   \n",
       "11          The bank vault was robust.       The bank robber was arrested.   \n",
       "12  He had to bank on her for support.          The bank was out of money.   \n",
       "13  He had to bank on her for support.       The bank robber was arrested.   \n",
       "14          The bank was out of money.       The bank robber was arrested.   \n",
       "\n",
       "    similarity  \n",
       "0     0.338063  \n",
       "1     0.494098  \n",
       "2     0.256140  \n",
       "3     0.469941  \n",
       "4     0.433940  \n",
       "5     0.523326  \n",
       "6     0.331584  \n",
       "7     0.512161  \n",
       "8     0.535616  \n",
       "9     0.416074  \n",
       "10    0.759213  \n",
       "11    0.798656  \n",
       "12    0.458184  \n",
       "13    0.381479  \n",
       "14    0.700466  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "\n",
    "list_of_sim= []\n",
    "for i in range(len(texts)-1):\n",
    "    for j in range(i+1,len(texts)):\n",
    "        text_1 = texts[i]\n",
    "        text_2 = texts[j]\n",
    "        embd_1 = target_word_embeddings[i]\n",
    "        embd_2 = target_word_embeddings[j]\n",
    "        cos_sim = 1 - cosine(embd_1.detach().numpy(), embd_2.detach().numpy())\n",
    "        list_of_sim.append([text_1, text_2, cos_sim])\n",
    "sims_df = pd.DataFrame(list_of_sim, columns=['text1', 'text2', 'similarity'])\n",
    "sims_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66eaab8",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 1?\n",
    "\n",
    "![](https://www.dropbox.com/s/02pizzlbl2qhnsx/weight_scheme.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6a77c",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 2?\n",
    "\n",
    "![](https://www.dropbox.com/s/pkgchdb8qu1tt26/steps.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5b5c5",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 3?\n",
    "\n",
    "![](https://www.dropbox.com/s/oai4l6yjs5tvq18/reweigh_sem_sim.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65953f",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 4?\n",
    "\n",
    "![](https://www.dropbox.com/s/oai4l6yjs5tvq18/reweigh_sem_sim.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde70812",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 5?\n",
    "\n",
    "![](https://www.dropbox.com/s/e6fva5rwfxq0sc5/self_attention.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89846d8f",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 6?\n",
    "![](https://www.dropbox.com/s/87vgwxurl9dvw96/attention_block_no_params.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a640d66",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 7?\n",
    "![](https://www.dropbox.com/s/y6e8k6re7zgauqd/attention_block_params.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2145c",
   "metadata": {},
   "source": [
    "### How does BERT Work: intuition - 8?\n",
    "![](https://www.dropbox.com/s/woqfyyh92rt72h9/multi_head_attension.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2cb9f",
   "metadata": {},
   "source": [
    "### Why use Multi-Head Attention?\n",
    "\n",
    "* Language is complex and ambiguous.\n",
    "\n",
    "  * Many rules and principles govern syntax, word formation, and other features.\n",
    "\n",
    "  * There are many exceptions to these rules.\n",
    "\n",
    "* Multiple attention heads are needed to capture the complexity of the language.\n",
    "\n",
    "  * Each head will \"focus\" on different aspects of the language, possibly on different regions.\n",
    "\n",
    "  \n",
    "\n",
    "![](https://www.dropbox.com/s/67qf3xrweu1p9tl/heads.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffe98e",
   "metadata": {},
   "source": [
    "### How does BERT Work: Interesting Reads?\n",
    "\n",
    "\n",
    "The Illustrated Transformer. An easy-to-follow blog post that is somewhat superficial.\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Transformers from scratch. A blog entry that is easy to and provides code to illustrate. https://peterbloem.nl/blog/transformers\n",
    "\n",
    "Attention is all you need. The paper that Introduced \"modern\" transformers as we know them\n",
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "An PyTorch annotated guide that explains the Attenion is All You nNeed paper \n",
    "http://nlp.seas.harvard.edu/2018/04/01/attention.html\n",
    "\n",
    "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned: https://aclanthology.org/P19-1580.pdf\n",
    "\n",
    "* An excellent explanation of how BERT is implemented: https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/#bertpooler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fafa3f",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "* Language models are typically trained on very large amounts of data.\n",
    "\n",
    "* Training can take weeks on very sophisticated architecture and cost very much $$$\n",
    "\n",
    "  * Training GPT-3 reportedly cost $12M for a single training run    \n",
    "  https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\n",
    "\n",
    "* The training is done on generic data\n",
    "  * Note optimized for a specific domain (ex. healthcare or physics)\n",
    "* Training can also be done on a generic task \n",
    "  * e.g. Masked Language modeling,(e.g., \"my [MASK] is John Doe\", \"name\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf23ae8",
   "metadata": {},
   "source": [
    "### Transfer Learning - cont'd\n",
    "\n",
    "* Transfer learning is the process of transferring knowledge from model A, to a model B\n",
    "\n",
    "  * Model B may be doing a task that's slightly different\n",
    "\n",
    "    * E.g., model A does NER on news items and model B does NER on invoices (company names, total-tax, item counts, etc..) \n",
    "\n",
    "    * Model A trained on English Wikipedia and model B on health care documents.\n",
    "\n",
    "    * Task can be different\n",
    "\n",
    "      * A does masked-language analysis, B does sentiment analysis\n",
    "\n",
    "* The knowledge acquired in model A is transferred to model B\n",
    "\n",
    "* Model B, typically needs a smaller dataset to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64262ce",
   "metadata": {},
   "source": [
    "### Transfer Learning - Cont'd\n",
    "\n",
    "* When training from scratch, all the model's weights are initialized randomly.\n",
    "\n",
    "* Transfer learning consists of \"continuing\" training with a new, smaller dataset\n",
    "\n",
    "  * Some approaches are used to force weights not to change too much.\n",
    "\n",
    "    * E.g., a much smaller learning rate or even freezing some layers and adding new ones that learn something new, \n",
    "\n",
    "* Since the pre-trained model was already trained on lots of data, the fine-tuning requires substantially fewer data to obtain reasonable results in less time and with fewer computational resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c755307",
   "metadata": {},
   "source": [
    "### Pretraining in Language Model \n",
    "\n",
    "* As we've seen with the Word2Vec model, pre-training can be self-supervised\n",
    "  * Dataset does not require human annotation\n",
    "  * Train on some task that allows the model to acquire some understanding of the language. Examples\n",
    "    * Predict the next word in a sentence.\n",
    "      * This is, for example, how GPT-2 was trained\n",
    "    * Masked Word Prediction\n",
    "      * BERT was trained on English Wikipedia and 11k books\n",
    "####### Next word prediction\n",
    "```\n",
    "Apple  -> will\n",
    "Apple will -> soon\n",
    "Apple will soon  -> allow\n",
    "Apple will soon allow -> customers\n",
    "Apple will soon allow customers -> to\n",
    "Apple will soon allow customers to -> fix\n",
    "Apple will soon allow customers to fix -> their\n",
    "Apple will soon allow customers to fix their -> devices\n",
    "Apple will soon allow customers to fix their devices -> .\n",
    "```\n",
    "####### Masked Word prediction\n",
    "```\n",
    "The company will [MASK] their earnings tomorrow.\n",
    "MASK= announce\n",
    "....\n",
    "```\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035bf80",
   "metadata": {},
   "source": [
    "### Transfer Learning for a Different Task\n",
    "\n",
    "![](https://www.dropbox.com/s/xt9io0croj9mked/transfer_learning.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48093ac",
   "metadata": {},
   "source": [
    "### Transfer Learning for a Different Task\n",
    "\n",
    "* The tasks should be as similar as possible\n",
    "\n",
    "  * Uses the same language\n",
    "\n",
    "  * Produce the same type of output (distributions over words)\n",
    "\n",
    "  * Etc.\n",
    "\n",
    "* The derived model leverages the language understanding acquired in the previous model\n",
    "\n",
    "    * Amazing, brilliant, fun, cool... convey somewhat similar meanings.\n",
    "\n",
    "    * When trained with \"such a fun movie\" -> \"positive\", it will learn that the sentence could have also been\n",
    "\n",
    "      * \"such an amazing movie\". \"such a brilliant movie\", \"such a cool movie\", etc.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
